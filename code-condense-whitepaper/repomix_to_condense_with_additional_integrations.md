# Repomix-to-Condense Pipeline: Compression Analysis & Tool Integrations

> **Research question:** How much does repomix losslessly condense? How much additional benefit would a code → repomix → zstd pipeline add, and what about the round-trip back? Then: a deeper dive on integrations with PPM, ast-grep, custom Makefiles, and other tools for polyglot repos.

---

## 1. How Much Does Repomix Condense?

Repomix operates at two distinct levels:

### 1a. Lossless Packing (default, no `--compress`)

Repomix concatenates all source files into a single structured document (XML, Markdown, JSON, or plain text). This step is **not compression** in the traditional sense — the output is typically *larger* than the sum of individual files because it adds:

- XML/Markdown wrapper tags per file
- Directory structure metadata
- File summary / token count header
- Security check metadata

**Estimated overhead:** +5–15% over raw file concatenation, depending on output style. XML adds the most overhead; plain text the least.

However, the *effective* reduction for AI consumption comes from:
- **Ignore filtering:** Stripping `node_modules/`, `dist/`, `.git/`, lockfiles, binaries — which in many repos account for 80–95% of total disk size
- **Comment removal** (`--remove-comments`): Typically saves 10–25% of source-only content
- **Empty line removal** (`--remove-empty-lines`): Saves another 3–8%

**Net result (lossless):** For a typical Node.js/Python project, repomix with `--remove-comments --remove-empty-lines` produces output that is roughly **60–80% of the raw source file content** (excluding already-ignored files). Relative to total repo size on disk (including node_modules, .git, etc.), the reduction is **90–98%**.

### 1b. Lossy Compression (`--compress`)

Using Tree-sitter, repomix extracts structural signatures and strips function bodies:

- **Preserves:** Function/method signatures, class structures, interface/type definitions, imports, exports, docstrings
- **Removes:** Function bodies, loop/conditional internals, local variable assignments

**Reported reduction:** ~70% token count reduction vs. full source. Combined with comment and empty-line removal, the total reduction from source → compressed repomix output is typically **75–85% fewer tokens**.

**Source:** [Repomix Code Compression Guide](https://repomix.com/guide/code-compress), repomix GitHub repository (22.1k stars, actively maintained)

---

## 2. The code → repomix → zstd Pipeline

### 2a. What zstd Adds on Top of Repomix Output

Repomix output is structured text (XML/Markdown) with high redundancy — repeated tag names, indentation patterns, and boilerplate. This makes it an excellent candidate for general-purpose compression.

**Estimated zstd ratios on repomix output (text-heavy, structured):**

| zstd Level | Compression Ratio | Compress Speed | Decompress Speed |
|---|---|---|---|
| `-1` (default) | 2.5–3.0x | ~500 MB/s | ~1500 MB/s |
| `-9` (balanced) | 3.0–3.5x | ~50 MB/s | ~1500 MB/s |
| `-19` (high) | 3.5–4.0x | ~5 MB/s | ~1500 MB/s |
| `--ultra -22` (max) | 3.8–4.2x | ~2 MB/s | ~1500 MB/s |

**Key insight:** Decompression speed is essentially constant across all levels (~1500 MB/s). This is critical for the use case of reloading condensed repos into AI tools.

**Reference benchmarks** (Silesia corpus, zstd 1.5.7, Core i7-9700K):
- zstd -1: ratio 2.896, 510 MB/s compress, 1550 MB/s decompress
- zstd --fast=1: ratio 2.439, 545 MB/s compress, 1850 MB/s decompress

Source: [facebook/zstd README](https://github.com/facebook/zstd), [zstd.net benchmarks](https://facebook.github.io/zstd/#benchmarks)

### 2b. Combined Pipeline Numbers

For a **10 MB polyglot source tree** (after .gitignore filtering):

| Stage | Size | Cumulative Reduction |
|---|---|---|
| Raw source files | 10 MB | — |
| repomix (lossless, `--remove-comments --remove-empty-lines`) | ~7 MB | 30% |
| repomix (`--compress`) | ~2.5 MB | 75% |
| + zstd -9 | ~0.7–0.9 MB | 91–93% |
| + zstd --ultra -22 | ~0.6–0.8 MB | 92–94% |

For comparison, raw source → zstd -9 (no repomix) would yield ~2.5–3.3 MB. The repomix `--compress` step provides the dominant reduction because it is *semantic* — it understands code structure — whereas zstd is purely statistical/lexical.

### 2c. The Round-Trip: code → repomix → zstd → unzstd → repomix-output

**zstd is fully lossless.** Decompressing a `.zst` file yields the exact repomix output byte-for-byte. So the round-trip question is really about repomix:

- **Without `--compress`:** The round-trip is lossless. The repomix XML/Markdown contains the full file contents. You can extract individual files from the output (though repomix doesn't provide an "unpack" tool — you'd need to parse the output format).
- **With `--compress`:** The round-trip is **lossy**. Function bodies are gone. You cannot reconstruct the original source from compressed repomix output. This is a one-way transformation suitable for AI analysis, not archival.

---

## 3. Deep Dive: Additional Tools & Integrations

### 3a. Prediction by Partial Matching (PPMd)

PPM is an adaptive statistical compression technique using Markov context models + arithmetic coding. PPMd (by Dmitry Shkarin) is the most practical implementation, used in RAR and 7-Zip.

**PPMd vs. zstd on text/code:**

On the Large Text Compression Benchmark (enwik9 — 1 GB of Wikipedia XML):

| Compressor | Compressed Size | Ratio | Compress Time | Memory |
|---|---|---|---|---|
| FreeArc PPMd (order 13, 1012 MB) | 175 MB | 5.7x | 1175s | 1046 MB |
| 7zip PPMd (order 10, 1630 MB) | 179 MB | 5.6x | 503s | 1630 MB |
| zstd --ultra -22 | 216 MB | 4.6x | 701s | 792 MB |
| xz -9 -e (LZMA2) | 248 MB | 4.0x | 2310s | 690 MB |
| bzip2 -9 | 254 MB | 3.9x | 379s | 8 MB |
| gzip -9 | 323 MB | 3.1x | 101s | 1.6 MB |

Source: [Matt Mahoney's Large Text Compression Benchmark](https://mattmahoney.net/dc/text.html)

**Key takeaway for code:** PPMd achieves 15–25% better compression than zstd's maximum on text-heavy content, but at the cost of:
- 5–10x slower decompression
- Higher memory requirements (1–2 GB vs. <1 GB)
- No streaming/dictionary support comparable to zstd

**Integration with repomix pipeline:**
```sh
# Active use (fast decompress for AI tools):
repomix --compress | zstd -9 > repo.xml.zst

# Archival (maximum squeeze):
repomix --compress -o repo.xml
7z a -m0=PPMd -mx=9 -mmem=1024m repo.7z repo.xml
```

**For polyglot codebases specifically:** PPMd excels because code has strong byte-level patterns (indentation, keyword repetition, naming conventions) that PPM's adaptive context modeling captures better than LZ77-family algorithms. The advantage grows with homogeneous code style.

### 3b. ast-grep (Structural Code Search & Rewriting)

**ast-grep** (12.6k GitHub stars, MIT license) is a Rust CLI tool for AST-based code structural search, lint, and rewriting. It uses Tree-sitter (the same parser Repomix uses for `--compress`).

**Relevance to compression pipelines:**

1. **Pre-compression normalization:** ast-grep can rewrite code patterns to canonical forms before compression, increasing redundancy and improving compression ratios:
   ```sh
   # Normalize all arrow functions to consistent style
   sg --pattern 'function $NAME($ARGS) { return $EXPR }' \
      --rewrite 'const $NAME = ($ARGS) => $EXPR' --lang ts
   ```

2. **Selective extraction:** Instead of repomix's all-or-nothing `--compress`, ast-grep can extract specific patterns:
   ```sh
   # Extract only exported API surfaces
   sg --pattern 'export $$$' --lang ts -r . > api-surface.txt
   ```

3. **Dead code elimination pre-pass:** Remove unused imports, unreachable code, and debug statements before packing to reduce content.

4. **Cross-language support:** ast-grep supports 20+ languages via Tree-sitter grammars — matching repomix's polyglot capability but with surgical precision.

**Pipeline integration:**
```sh
# Normalize → strip dead code → pack → compress
sg --pattern 'console.log($$$)' --rewrite '' --lang ts -r src/
repomix --compress --remove-comments -o packed.xml src/
zstd -9 packed.xml
```

### 3c. Custom Makefiles for Polyglot Pipelines

For repos with mixed languages and file types, a Makefile can orchestrate per-filetype optimal compression:

```makefile
SRCDIR := src
OUTDIR := .condense
ZSTD_LEVEL := 9

# Separate strategies by file type
CODE_FILES := $(shell find $(SRCDIR) -name '*.ts' -o -name '*.py' -o -name '*.rs' -o -name '*.go' -o -name '*.java')
CONFIG_FILES := $(shell find $(SRCDIR) -name '*.json' -o -name '*.yaml' -o -name '*.toml' -o -name '*.xml')
TEXT_FILES := $(shell find $(SRCDIR) -name '*.md' -o -name '*.txt' -o -name '*.rst')
IMAGE_FILES := $(shell find $(SRCDIR) -name '*.png' -o -name '*.jpg' -o -name '*.svg')

# Code: semantic compress via repomix
$(OUTDIR)/code.xml.zst: $(CODE_FILES)
	repomix --compress --remove-comments --include "**/*.{ts,py,rs,go,java}" \
		-o $(OUTDIR)/code.xml
	zstd -$(ZSTD_LEVEL) --rm $(OUTDIR)/code.xml

# Config: lossless pack (structure matters)
$(OUTDIR)/config.xml.zst: $(CONFIG_FILES)
	repomix --remove-empty-lines --include "**/*.{json,yaml,toml,xml}" \
		-o $(OUTDIR)/config.xml
	zstd -$(ZSTD_LEVEL) --rm $(OUTDIR)/config.xml

# Text: PPMd for maximum ratio (compress once, read rarely)
$(OUTDIR)/docs.7z: $(TEXT_FILES)
	repomix --include "**/*.{md,txt,rst}" -o $(OUTDIR)/docs.xml
	7z a -m0=PPMd -mx=9 $(OUTDIR)/docs.7z $(OUTDIR)/docs.xml
	rm $(OUTDIR)/docs.xml

# Images: already compressed, just archive
$(OUTDIR)/assets.tar.zst: $(IMAGE_FILES)
	tar cf - $(IMAGE_FILES) | zstd -1 > $(OUTDIR)/assets.tar.zst

condense: $(OUTDIR)/code.xml.zst $(OUTDIR)/config.xml.zst $(OUTDIR)/docs.7z $(OUTDIR)/assets.tar.zst

clean:
	rm -rf $(OUTDIR)

.PHONY: condense clean
```

**Why per-type strategies matter:**

| File Type | Best Strategy | Reason |
|---|---|---|
| Code (.ts, .py, .rs) | repomix `--compress` + zstd | AST extraction removes 70% before statistical compression |
| Config (.json, .yaml) | repomix lossless + zstd | Structure must be preserved; high redundancy benefits zstd |
| Text (.md, .txt) | repomix + PPMd | Natural language is PPMd's sweet spot (15–25% better than zstd) |
| Images (.png, .jpg) | tar + zstd -1 | Already compressed; fast archive only |
| SVG | repomix lossless + zstd -19 | XML text; compresses very well |

### 3d. Dictionary Training for Polyglot Repos

zstd's **dictionary training** feature is underexplored for code compression:

```sh
# Train a dictionary on your codebase's file samples
zstd --train src/**/*.ts src/**/*.py -o code.dict

# Compress with the trained dictionary
zstd -D code.dict -9 packed-output.xml
```

For repos with consistent coding style, a trained dictionary can improve compression ratios by **10–30% on small-to-medium files** (under 100 KB). The gains diminish for larger files where zstd's adaptive modeling catches up.

**Best for:** Microservices repos, monorepos with many similarly-structured small files, configuration file collections.

### 3e. BWT (Burrows-Wheeler Transform) Preprocessing

bzip2 uses BWT + MTF + Huffman. While generally slower than zstd, BWT excels on highly repetitive text like code.

From the Large Text Compression Benchmark (enwik9, 1 GB Wikipedia XML):
- **bzip2 -9:** compressed to 254 MB (3.9x ratio), 379s compress time, 8 MB memory
- **zstd --ultra -22:** compressed to 216 MB (4.6x ratio), 701s compress time, 792 MB memory
- **PPMd (7zip, order 10):** compressed to 179 MB (5.6x ratio), 503s compress time, 1630 MB memory

Source: [mattmahoney.net/dc/text.html](https://mattmahoney.net/dc/text.html)

BWT could theoretically be used as a preprocessing stage before entropy coding, but no production tool currently chains BWT → zstd cleanly.

### 3f. Tree-sitter Grammar-Aware Tokenization

Both repomix and ast-grep use Tree-sitter for parsing. A potential pipeline enhancement:

1. **Tree-sitter parse** → extract AST
2. **AST normalization** → canonical variable names, consistent formatting
3. **AST serialization** → compact binary or S-expression format
4. **zstd compression** → on the normalized output

This theoretical pipeline could achieve better compression than repomix's current approach because AST normalization eliminates cosmetic variation (naming, whitespace, brace style) that wastes entropy.

No production tool implements this full pipeline today, but the components exist:
- Tree-sitter: 23+ language grammars
- ast-grep: structural rewriting
- repomix: packing and Tree-sitter integration
- zstd: final compression

---

## 4. Summary: Optimal Pipelines by Use Case

| Use Case | Pipeline | Expected Reduction |
|---|---|---|
| **AI chat (fast reload)** | repomix `--compress` → zstd -3 | ~90% tokens, instant decompress |
| **AI deep analysis** | repomix (lossless) → zstd -9 | ~70% size, preserves all logic |
| **Cold archival** | repomix → PPMd via 7z | ~93–95% size, slow decompress |
| **Polyglot repo (mixed types)** | Makefile with per-type strategy | ~90–95% overall |
| **Monorepo (many small files)** | zstd dictionary training + repomix | ~92% with trained dict |
| **CI artifact storage** | repomix → zstd --fast=3 | ~80% size, minimal CPU cost |

---

## Sources

- [Repomix documentation — Code Compression](https://repomix.com/guide/code-compress)
- [Repomix configuration reference](https://repomix.com/guide/configuration)
- [facebook/zstd — GitHub](https://github.com/facebook/zstd) (26.7k stars, v1.5.7)
- [Zstandard benchmarks](https://facebook.github.io/zstd/#benchmarks)
- [Matt Mahoney — Large Text Compression Benchmark](https://mattmahoney.net/dc/text.html)
- [ast-grep/ast-grep — GitHub](https://github.com/ast-grep/ast-grep) (12.6k stars, v0.41.0)
- [Wikipedia — Prediction by Partial Matching](https://en.wikipedia.org/wiki/Prediction_by_partial_matching)
- [Repomix GitHub issues — compress/token discussions](https://github.com/yamadashy/repomix/issues?q=compress+token)

---

## Related Research

- [OTEL Telemetry Data Compression](otel_telemetry_data_compression.md) — Compression best practices for OpenTelemetry traces, metrics, and logs; OTLP protocol compression; ClickHouse codecs; collector and storage-level strategies.
- [SQL & KV Data Storage Compression](sql_kv_data_compression.md) — Compression best practices for SQL databases (PostgreSQL, ClickHouse, MySQL) and KV stores (RocksDB, Redis, Cloudflare KV); per-column codec selection; write amplification tradeoffs.

---

## Appendix: Practical Repomix Pipeline Choices (2025–2026 Era)

In practice for repomix pipelines (2025–2026 era):

Most people choose zstd (often level 3–9 for balance, or `--ultra -19`–`22` when squeezing harder) because the decompression speed advantage is huge when reloading the condensed repo into tools like Continue.dev, Cursor, Aider, or Claude.

PPMd shines in offline archival scenarios (e.g., yearly repo snapshots, cold storage) where you compress once and rarely decompress.

**Hybrid tip:** `repomix --compress` → zstd (fast everyday) for active use; `repomix --compress` → PPMd (via `7z a -m0=PPMd -mx=9 archive.7z`) for maximum squeeze when archiving.
